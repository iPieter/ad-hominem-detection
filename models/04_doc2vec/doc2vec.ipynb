{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Train and Test\n",
    "The idea is to ultimately create a module that takes the data frame and return, instead of the body text, retuns a vector for each paragraph input (data input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train and test data\n",
    "\n",
    "* Data (train and test): [Cleaned reddit dataset](../../data/ad_hominem/ad_hominems_cleaned.csv), the data will be separated into test and train in a 70-30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "data = pd.read_csv(\"../../data/ad_hominem/ad_hominems_cleaned.csv\")\n",
    "train_data, test_data = np.split(data.sample(frac=1), [int(.7*len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a function to open the train/test file (with latin encoding)\n",
    "* Read the file line-by-line\n",
    "* Pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "* Return a list of words.\n",
    "Note that, for the data frame (corpus), each row constitutes a single document and the length of row entry (i.e., document) can vary. Also, to train the model, we'll need to associate a tag/number with each document of the training corpus. In our case, the tag is simply the index for the data frame (row number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(df, tokens_only=False):\n",
    "    #with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "    for i, line in df.iterrows():\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(str(line[\"reddit_ad_hominem.body\"]))\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(str(line[\"reddit_ad_hominem.body\"])), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(train_data))\n",
    "test_corpus = list(read_corpus(test_data, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus (both in the data frame and the generated corpus to see the differences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23552</th>\n",
       "      <td>be cute. Indulge in yourself. In the society we exist in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17342</th>\n",
       "      <td>There is no such equivalent.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reddit_ad_hominem.body\n",
       "23552   be cute. Indulge in yourself. In the society we exist in\n",
       "17342  There is no such equivalent.                             "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.DataFrame(train_data.loc[:, \"reddit_ad_hominem.body\"])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['be', 'cute', 'indulge', 'in', 'yourself', 'in', 'the', 'society', 'we', 'exist', 'in'], tags=[23552]),\n",
       " TaggedDocument(words=['there', 'is', 'no', 'such', 'equivalent'], tags=[17342])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8806</th>\n",
       "      <td>Everyone talks about how much of a joke Donald Trump is. Just today there was an askreddit thread about how people are sure he's just running as a joke. I don't get it though. In my opinion, he's my favorite Republican in years.Here are some views he has that are more progressive than the rest of his party.1. He is not strong against gay marriage/abortion. In fact, up until recently, he was publicly in favor of these things. Pretty obvious these points are not a big part of his campaign.2. He is anti Super-PAC. Just like Bernie, he is not taking outside donations from Wall Street or big name corporations. He is willing to address the issue that corporations have too much power over politicians. This is HUGE in my opinion.3. His foreign policy is very sound. Let the Germans deal with Crimea. Let the Russians deal with ISIS. We don't need to be the police of the world anymore. A lot of Republicans seem to act like we're still in the Cold War and at war with Russia. I honestly believe that Trump and Putin could get along. China is the biggest economic enemy at this point. Trump acknowledges this. 4. US-China trade reform. China does seem to be breaking an incredible amount of WTO rules. I'm not sure that he will be able to bring them to the table to negotiate. However, at least he has a cohesive plan.Conclusion: I will not be voting for Donald Trump. Issues like global warming are too important to me for me to ever vote for a Republican candidate. However, the general idea is that Trump is a joke. This is absurd. If anything he's a progressive figure for the Republican party. He's infinitely better than Carson in my opinion. People say Trump scares them... Carson scares me much more.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10448</th>\n",
       "      <td>What ones am I missing?????? I acknowledged the main ones you keep bringing up, medical, insurance, and tax. I literally mentioned these points in my FLIPPIN POST. I'm asking you to bring up a better benefit than what I already flippin mentioned. Those 1500+ laws you keep talking about mostly flippin apply to the main points you keep bringing up. Property, children, medical, insurance, tax, alimony, etc. Jesus.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              reddit_ad_hominem.body\n",
       "8806   Everyone talks about how much of a joke Donald Trump is. Just today there was an askreddit thread about how people are sure he's just running as a joke. I don't get it though. In my opinion, he's my favorite Republican in years.Here are some views he has that are more progressive than the rest of his party.1. He is not strong against gay marriage/abortion. In fact, up until recently, he was publicly in favor of these things. Pretty obvious these points are not a big part of his campaign.2. He is anti Super-PAC. Just like Bernie, he is not taking outside donations from Wall Street or big name corporations. He is willing to address the issue that corporations have too much power over politicians. This is HUGE in my opinion.3. His foreign policy is very sound. Let the Germans deal with Crimea. Let the Russians deal with ISIS. We don't need to be the police of the world anymore. A lot of Republicans seem to act like we're still in the Cold War and at war with Russia. I honestly believe that Trump and Putin could get along. China is the biggest economic enemy at this point. Trump acknowledges this. 4. US-China trade reform. China does seem to be breaking an incredible amount of WTO rules. I'm not sure that he will be able to bring them to the table to negotiate. However, at least he has a cohesive plan.Conclusion: I will not be voting for Donald Trump. Issues like global warming are too important to me for me to ever vote for a Republican candidate. However, the general idea is that Trump is a joke. This is absurd. If anything he's a progressive figure for the Republican party. He's infinitely better than Carson in my opinion. People say Trump scares them... Carson scares me much more.\n",
       "10448  What ones am I missing?????? I acknowledged the main ones you keep bringing up, medical, insurance, and tax. I literally mentioned these points in my FLIPPIN POST. I'm asking you to bring up a better benefit than what I already flippin mentioned. Those 1500+ laws you keep talking about mostly flippin apply to the main points you keep bringing up. Property, children, medical, insurance, tax, alimony, etc. Jesus.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_data.loc[:, \"reddit_ad_hominem.body\"])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['everyone', 'talks', 'about', 'how', 'much', 'of', 'joke', 'donald', 'trump', 'is', 'just', 'today', 'there', 'was', 'an', 'askreddit', 'thread', 'about', 'how', 'people', 'are', 'sure', 'he', 'just', 'running', 'as', 'joke', 'don', 'get', 'it', 'though', 'in', 'my', 'opinion', 'he', 'my', 'favorite', 'republican', 'in', 'years', 'here', 'are', 'some', 'views', 'he', 'has', 'that', 'are', 'more', 'progressive', 'than', 'the', 'rest', 'of', 'his', 'party', 'he', 'is', 'not', 'strong', 'against', 'gay', 'marriage', 'abortion', 'in', 'fact', 'up', 'until', 'recently', 'he', 'was', 'publicly', 'in', 'favor', 'of', 'these', 'things', 'pretty', 'obvious', 'these', 'points', 'are', 'not', 'big', 'part', 'of', 'his', 'campaign', 'he', 'is', 'anti', 'super', 'pac', 'just', 'like', 'bernie', 'he', 'is', 'not', 'taking', 'outside', 'donations', 'from', 'wall', 'street', 'or', 'big', 'name', 'corporations', 'he', 'is', 'willing', 'to', 'address', 'the', 'issue', 'that', 'corporations', 'have', 'too', 'much', 'power', 'over', 'politicians', 'this', 'is', 'huge', 'in', 'my', 'opinion', 'his', 'foreign', 'policy', 'is', 'very', 'sound', 'let', 'the', 'germans', 'deal', 'with', 'crimea', 'let', 'the', 'russians', 'deal', 'with', 'isis', 'we', 'don', 'need', 'to', 'be', 'the', 'police', 'of', 'the', 'world', 'anymore', 'lot', 'of', 'republicans', 'seem', 'to', 'act', 'like', 'we', 're', 'still', 'in', 'the', 'cold', 'war', 'and', 'at', 'war', 'with', 'russia', 'honestly', 'believe', 'that', 'trump', 'and', 'putin', 'could', 'get', 'along', 'china', 'is', 'the', 'biggest', 'economic', 'enemy', 'at', 'this', 'point', 'trump', 'acknowledges', 'this', 'us', 'china', 'trade', 'reform', 'china', 'does', 'seem', 'to', 'be', 'breaking', 'an', 'incredible', 'amount', 'of', 'wto', 'rules', 'not', 'sure', 'that', 'he', 'will', 'be', 'able', 'to', 'bring', 'them', 'to', 'the', 'table', 'to', 'negotiate', 'however', 'at', 'least', 'he', 'has', 'cohesive', 'plan', 'conclusion', 'will', 'not', 'be', 'voting', 'for', 'donald', 'trump', 'issues', 'like', 'global', 'warming', 'are', 'too', 'important', 'to', 'me', 'for', 'me', 'to', 'ever', 'vote', 'for', 'republican', 'candidate', 'however', 'the', 'general', 'idea', 'is', 'that', 'trump', 'is', 'joke', 'this', 'is', 'absurd', 'if', 'anything', 'he', 'progressive', 'figure', 'for', 'the', 'republican', 'party', 'he', 'infinitely', 'better', 'than', 'carson', 'in', 'my', 'opinion', 'people', 'say', 'trump', 'scares', 'them', 'carson', 'scares', 'me', 'much', 'more'], ['what', 'ones', 'am', 'missing', 'acknowledged', 'the', 'main', 'ones', 'you', 'keep', 'bringing', 'up', 'medical', 'insurance', 'and', 'tax', 'literally', 'mentioned', 'these', 'points', 'in', 'my', 'flippin', 'post', 'asking', 'you', 'to', 'bring', 'up', 'better', 'benefit', 'than', 'what', 'already', 'flippin', 'mentioned', 'those', 'laws', 'you', 'keep', 'talking', 'about', 'mostly', 'flippin', 'apply', 'to', 'the', 'main', 'points', 'you', 'keep', 'bringing', 'up', 'property', 'children', 'medical', 'insurance', 'tax', 'alimony', 'etc', 'jesus']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain any tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "### Instantiate a Doc2Vec Object \n",
    "Doc2Vec model with:\n",
    "* Vector size with 500 words\n",
    "* Iterating over the training corpus 10 times (More iterations take more time and eventually reach a point of diminishing returns)\n",
    "* Minimum word count set to 20 (discard words with very few occurrences)\n",
    "\n",
    "Note: retaining infrequent words can often make a model worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  model = gensim.models.doc2vec.Doc2Vec(vector_size=500, min_count=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via `model.wv.vocab`) of all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word `penalty`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 1.79 s, total: 2min 46s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00985859e-02, -1.66647714e-02, -1.78537089e-02, -1.02618933e-02,\n",
       "       -7.61404494e-03,  5.06948819e-03,  5.99262584e-03, -3.03212907e-02,\n",
       "       -1.11470679e-02,  3.84169258e-02, -3.12466584e-02, -2.09496589e-03,\n",
       "        4.69029834e-03,  9.80205461e-03, -6.78870920e-03,  1.12740705e-02,\n",
       "       -1.09866410e-02,  2.90585845e-03, -1.07265841e-02,  1.42388362e-02,\n",
       "       -3.97526985e-03,  8.75213835e-03,  2.64565833e-03, -1.09908134e-02,\n",
       "        1.18182003e-02, -1.23832384e-02, -2.68010106e-02,  7.90684298e-03,\n",
       "       -4.14244318e-03, -1.87092591e-02, -2.19194614e-03,  1.81097165e-02,\n",
       "        3.81297283e-02,  1.96947437e-02, -8.69639218e-03, -3.35553102e-02,\n",
       "        2.84875222e-02, -4.21259105e-02,  1.28740892e-02, -1.71385948e-02,\n",
       "       -1.82706583e-02,  3.29706334e-02,  3.66269425e-02,  1.12076895e-02,\n",
       "        5.22379158e-03, -2.38496549e-02, -5.06996643e-03,  1.18713723e-02,\n",
       "        3.02123148e-02, -3.39984633e-02,  1.21895531e-02, -1.39129348e-02,\n",
       "       -1.12414807e-02, -1.90948565e-02, -2.85116434e-02, -2.59658583e-02,\n",
       "       -1.53542040e-02,  9.78445448e-03,  1.89084727e-02,  1.19649339e-02,\n",
       "       -4.29921858e-02,  1.51904440e-02, -2.24549528e-02,  7.12694786e-03,\n",
       "        2.86871772e-02,  7.09733414e-03, -9.65318084e-03, -1.40530746e-02,\n",
       "        7.58051232e-04,  8.62959586e-03,  1.00252954e-02, -1.18672028e-02,\n",
       "       -1.43716941e-02,  2.88030668e-03,  3.55522856e-02,  1.04996078e-02,\n",
       "       -9.01285745e-03, -5.51885478e-02, -4.50207181e-02, -2.49635447e-02,\n",
       "       -1.73019357e-02,  4.96564955e-02, -3.39766964e-02, -2.89005530e-03,\n",
       "       -2.27449741e-02, -3.78704979e-03, -2.23204046e-02, -1.38173271e-02,\n",
       "        2.15633232e-02,  1.05265062e-03, -1.52855050e-02, -3.46982256e-02,\n",
       "        1.85204763e-02, -8.27581994e-03, -6.36984943e-04, -1.65265277e-02,\n",
       "        1.84560400e-02, -1.61477365e-02,  1.05949147e-02, -1.85600594e-02,\n",
       "        4.06023441e-03,  9.83236590e-04,  3.87400128e-02, -2.30340324e-02,\n",
       "        5.71020041e-03,  3.14276814e-02,  2.82583050e-02, -6.62496872e-03,\n",
       "        3.37533206e-02, -3.96057554e-02, -1.08550517e-02,  2.94468440e-02,\n",
       "       -1.47950929e-02, -9.94252320e-03, -6.76563848e-03, -2.56498158e-02,\n",
       "        5.97101860e-02,  1.11528505e-02,  2.10090782e-02, -1.35964167e-03,\n",
       "        1.39796976e-02, -1.98033713e-02,  3.47434171e-02,  9.37350467e-03,\n",
       "       -3.59559170e-04,  4.27147932e-02, -6.58676624e-02,  3.89505103e-02,\n",
       "        1.91917513e-02,  2.22086105e-02,  6.21800907e-02,  4.07399759e-02,\n",
       "        1.68100987e-02, -1.16703399e-02,  5.32181114e-02, -1.59980804e-02,\n",
       "        5.13246842e-02,  3.68920565e-02,  8.36929365e-04, -6.13731192e-03,\n",
       "        3.02402712e-02, -7.68012833e-04, -1.41999591e-02, -1.37663335e-02,\n",
       "        2.95811612e-03,  2.74774712e-02,  2.46611498e-02,  2.99359467e-02,\n",
       "       -2.22793240e-02, -8.84094555e-03,  2.01259926e-02, -6.40684087e-03,\n",
       "        1.41634129e-03,  7.51281530e-03, -1.80873573e-02,  2.27371641e-02,\n",
       "       -1.50485542e-02,  2.26888945e-03,  7.28357630e-03,  1.26185967e-02,\n",
       "       -3.10261138e-02, -2.64209118e-02, -8.48012976e-03,  3.40801477e-02,\n",
       "        2.73818080e-03,  3.21917683e-02,  3.61116044e-02, -9.72882658e-03,\n",
       "        4.49820869e-02, -7.11130723e-03, -1.44935269e-02,  1.77486409e-02,\n",
       "       -1.06504885e-02, -1.97131373e-02,  1.73482224e-02, -2.55081411e-02,\n",
       "        2.03919820e-02, -2.14774050e-02,  9.79118329e-03,  3.16699184e-02,\n",
       "       -2.62311641e-02,  9.29339882e-03,  2.17943378e-02, -1.17811817e-03,\n",
       "       -2.09894888e-02, -4.65856818e-03, -4.07743175e-03,  1.55585753e-02,\n",
       "       -2.62918733e-02, -1.30554503e-02, -1.69098787e-02, -2.11616512e-02,\n",
       "       -5.63644804e-03,  5.63252456e-02,  1.93664264e-02, -2.44051050e-02,\n",
       "        5.61049730e-02,  2.11113412e-02, -3.40950862e-02,  1.07112685e-02,\n",
       "        2.14438196e-02, -1.88798644e-02,  4.92611388e-03,  3.29715908e-02,\n",
       "       -6.03770465e-03,  1.91909596e-02,  2.36083269e-02,  2.42127944e-02,\n",
       "       -1.33277085e-02, -1.45344827e-02,  1.59922428e-02,  2.44611464e-02,\n",
       "       -2.18874458e-02, -1.42626278e-02, -1.45762796e-02, -2.54611168e-02,\n",
       "       -4.18750159e-02,  7.82843903e-02,  1.89655479e-02, -6.07828051e-03,\n",
       "       -1.02043496e-02, -2.05144696e-02, -3.12388898e-03, -5.33377901e-02,\n",
       "       -2.27492892e-05, -4.47640568e-03,  8.31957441e-03, -1.98981818e-02,\n",
       "       -2.17429455e-02,  2.03180499e-02, -1.03575047e-02, -2.15911474e-02,\n",
       "       -2.39589252e-02,  8.73810053e-03, -2.80698668e-02,  3.49470414e-02,\n",
       "        2.46138219e-02,  3.75440679e-02,  2.99457721e-02, -1.63559578e-02,\n",
       "        2.14821119e-02,  6.11303113e-02, -2.01856475e-02, -2.72863973e-02,\n",
       "        1.16167441e-02,  5.31802047e-03, -3.81182171e-02, -2.48239096e-02,\n",
       "       -1.48822945e-02, -6.59536757e-03, -2.07173079e-02, -2.74881572e-02,\n",
       "       -7.46266590e-03,  4.23427150e-02, -1.15978001e-02,  6.51285723e-02,\n",
       "        1.56713221e-02, -9.30352323e-03,  2.22626384e-02, -5.45742689e-03,\n",
       "        8.08909256e-03, -4.52935090e-03,  6.43894151e-02,  5.98443090e-04,\n",
       "        2.45223045e-02,  3.09552774e-02,  5.85974306e-02,  2.61375383e-02,\n",
       "        1.90969761e-02,  2.42251344e-02, -1.10903140e-02, -2.13430971e-02,\n",
       "        2.61981320e-02, -2.50185635e-02, -4.49372595e-03, -1.41369947e-03,\n",
       "       -2.74683274e-02,  2.85248877e-03, -1.62788853e-02, -1.28084477e-02,\n",
       "       -3.89481187e-02, -3.59905437e-02,  3.71888205e-02,  1.91505637e-03,\n",
       "        8.60282802e-04,  4.36084792e-02, -1.44854072e-03,  1.75686106e-02,\n",
       "       -2.10967511e-02,  3.38567281e-03,  5.12298848e-03, -2.11822987e-02,\n",
       "       -1.70658529e-02, -4.70042117e-02,  1.30011123e-02, -1.56654343e-02,\n",
       "        6.73636002e-03,  2.98096351e-02,  4.31637559e-03, -2.40105540e-02,\n",
       "        2.50593182e-02, -4.03633602e-02,  3.09402272e-02,  3.75311524e-02,\n",
       "        4.05267440e-03,  1.42039207e-03, -1.44905215e-02,  2.56709717e-02,\n",
       "        1.05040036e-02, -7.04904273e-03, -2.68005654e-02, -2.09436025e-02,\n",
       "        5.15529215e-02,  7.01920222e-03,  6.27541170e-02, -1.76652819e-02,\n",
       "        1.33698760e-02, -5.56521863e-03,  1.38688181e-02,  2.48147268e-02,\n",
       "       -1.68529761e-04, -9.54207685e-03,  1.07715102e-02, -2.00041337e-03,\n",
       "        5.07472199e-04, -2.14737523e-02,  4.67114598e-02, -4.96519031e-03,\n",
       "        1.90442372e-02,  1.75259002e-02, -6.74757315e-03,  4.14064378e-02,\n",
       "        2.18365174e-02,  1.51141398e-02, -1.25538679e-02, -3.14207487e-02,\n",
       "        2.90205386e-02, -2.84186080e-02, -3.83880213e-02,  1.53041640e-02,\n",
       "        4.43410054e-02, -2.76805628e-02,  1.87763255e-02, -7.34543800e-02,\n",
       "        1.42406998e-03,  3.77094466e-03, -2.45165396e-02, -5.90549894e-02,\n",
       "       -3.48053500e-02,  4.46674833e-03, -4.18279059e-02, -1.80285741e-02,\n",
       "       -2.07796320e-02,  3.00069596e-03, -1.63898605e-03,  2.03908682e-02,\n",
       "       -1.25519484e-02,  3.85047235e-02, -2.44372059e-02, -5.88419242e-03,\n",
       "        5.36987111e-02, -5.63931977e-03, -1.27141355e-02, -3.13943322e-03,\n",
       "       -1.37332249e-02,  1.41915791e-02,  3.15958969e-02,  3.73055041e-03,\n",
       "        3.54989283e-02, -9.17289127e-03, -1.11128241e-02, -6.15146011e-04,\n",
       "        3.21342684e-02,  1.44591480e-02,  1.38833458e-02,  2.69053113e-02,\n",
       "        2.93636043e-03, -8.99248943e-03,  7.62370136e-03, -4.31193300e-02,\n",
       "        3.10991909e-02, -1.59233753e-02,  8.82239547e-03,  9.46841110e-03,\n",
       "        1.39647420e-03, -3.22452635e-02,  1.90643650e-02,  8.85236170e-03,\n",
       "        2.19831187e-02, -4.27468643e-02,  2.99896067e-03, -9.63888131e-03,\n",
       "        1.47310514e-02, -3.02476715e-02, -1.29329888e-02,  1.83782564e-03,\n",
       "       -1.42114479e-02,  3.19028348e-02,  1.98544562e-02, -3.27035300e-02,\n",
       "       -2.81725544e-02, -8.54377355e-03, -2.06216983e-02, -3.00227180e-02,\n",
       "        3.58798839e-02,  5.85155329e-04, -2.71877227e-03,  1.67105272e-02,\n",
       "        1.50650451e-02, -1.22227604e-02, -1.18512828e-02,  3.13289277e-02,\n",
       "        1.33766551e-02,  1.82947144e-02,  4.01426281e-04,  1.24870464e-02,\n",
       "        8.99131969e-03,  2.45991237e-02,  1.31931733e-02,  4.59305979e-02,\n",
       "        1.81663837e-02,  1.24988817e-02, -5.01038157e-04, -4.68880236e-02,\n",
       "        1.55879073e-02, -2.89316047e-02,  2.06213165e-02,  1.81260202e-02,\n",
       "       -2.29050443e-02,  2.96029523e-02,  3.31616625e-02,  2.27402570e-03,\n",
       "        4.45134044e-02,  1.07508805e-02,  6.24363497e-03,  2.45647691e-02,\n",
       "        3.80968787e-02,  1.59924664e-02, -1.53086893e-02, -3.51711288e-02,\n",
       "        2.08555311e-02,  3.35371643e-02,  7.93594494e-03, -2.36582048e-02,\n",
       "       -2.10521724e-02, -1.91614386e-02,  1.61790121e-02,  2.75522452e-02,\n",
       "       -8.73160083e-03,  1.52332261e-02,  1.83672234e-02,  2.98142750e-02,\n",
       "       -1.61224697e-02, -1.73584241e-02, -2.39372952e-03,  5.90981450e-03,\n",
       "       -2.89875828e-02,  2.96404073e-03, -3.94872241e-02,  2.58679930e-02,\n",
       "       -1.98870897e-02, -2.59285197e-02,  1.92535557e-02,  4.65907156e-02,\n",
       "        6.29286021e-02, -5.87588735e-03, -1.02861016e-03,  1.41876633e-03,\n",
       "        2.87979003e-02, -1.33365078e-03,  4.82913442e-02,  2.70741563e-02,\n",
       "        1.38248075e-02, -3.03056557e-03,  1.55718410e-02, -8.47441610e-03,\n",
       "        8.35176371e-03,  1.42416414e-02,  4.29886999e-03,  1.37138665e-02,\n",
       "       -2.31900457e-02, -4.91229584e-03,  4.99386387e-03, -2.55596898e-02,\n",
       "       -5.80375530e-02,  2.17113625e-02, -5.06793940e-03, -1.98398833e-03,\n",
       "        4.84840712e-03,  2.24032328e-02, -1.15046548e-02, -4.11496527e-04,\n",
       "       -1.44028207e-02,  1.72026325e-02, -3.98534350e-03,  8.39976780e-03,\n",
       "        8.62942170e-03, -1.71667766e-02,  6.04361203e-03,  7.30527611e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `infer_vector()` takes a list of *string tokens*\n",
    "* Input should be tokenized prior to inference\n",
    "    * Here the test set is already tokenized (in `test_corpus = list(read_corpus(test_data, tokens_only=True))`)\n",
    "    \n",
    "Note: algorithms use internal randomization, so repeated inferences of the same text will return slightly different vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "Deleting training data from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"reddit-doc2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load the model:\n",
    "\n",
    "`model = Doc2Vec.load(fname)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To use model for inference:\n",
    "\n",
    "`vector = model.infer_vector([\"tokenized\", \"input\", \"string\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "### To load the model:\n",
    "* `model = Doc2Vec.load(fname)` (not required here)\n",
    "### To use model for inference:\n",
    "* `vector = model.infer_vector([\"tokenized\", \"input\", \"string\"])`\n",
    "    #### To tokenize:\n",
    "    * `list(read_corpus(df, tokens_only=False))` (used earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized test sample:\n",
      "['what', 'ones', 'am', 'missing', 'acknowledged', 'the', 'main', 'ones', 'you', 'keep', 'bringing', 'up', 'medical', 'insurance', 'and', 'tax', 'literally', 'mentioned', 'these', 'points', 'in', 'my', 'flippin', 'post', 'asking', 'you', 'to', 'bring', 'up', 'better', 'benefit', 'than', 'what', 'already', 'flippin', 'mentioned', 'those', 'laws', 'you', 'keep', 'talking', 'about', 'mostly', 'flippin', 'apply', 'to', 'the', 'main', 'points', 'you', 'keep', 'bringing', 'up', 'property', 'children', 'medical', 'insurance', 'tax', 'alimony', 'etc', 'jesus']\n",
      "\n",
      "Inferred vector:\n",
      "[-1.46771912e-02 -3.45688686e-02  4.79957797e-02  8.53247121e-02\n",
      " -2.08942089e-04 -2.52568126e-02  3.83563042e-02  3.92340496e-02\n",
      " -2.71544382e-02 -4.67369370e-02 -6.32333755e-02  1.71506196e-01\n",
      "  1.35483861e-01  3.58873457e-02  2.82059424e-03  1.20748810e-01\n",
      " -9.56936702e-02 -8.10178593e-02  3.72729376e-02 -3.99550535e-02\n",
      " -2.03975216e-02  1.45050185e-02  1.30063161e-01  2.66129300e-02\n",
      "  3.67466658e-02  3.36539298e-02 -5.28044701e-02  2.45529916e-02\n",
      " -3.43340524e-02 -9.97737497e-02 -6.53751343e-02  5.92786111e-02\n",
      "  9.98782218e-02 -5.16309515e-02  4.11662944e-02 -7.29940608e-02\n",
      "  5.03114685e-02 -1.12103701e-01  1.37859836e-01 -4.74842191e-02\n",
      " -7.36905411e-02  1.18878987e-02  9.36003253e-02 -6.49115909e-03\n",
      "  1.43300584e-02 -1.10285088e-01 -8.89817700e-02  1.70505550e-02\n",
      "  1.05304949e-01 -8.64536166e-02  7.54039809e-02  6.58850297e-02\n",
      "  1.95944794e-02 -4.92183398e-03 -1.95371181e-01 -1.02433600e-01\n",
      " -5.69225475e-02  3.71785015e-02  1.71513498e-01 -1.49794919e-02\n",
      " -5.59760891e-02  1.22206680e-01  8.50145817e-02  1.65088978e-02\n",
      " -7.19587132e-02  1.91584080e-02  6.87439414e-03 -1.24409525e-02\n",
      " -1.08755067e-01  1.04038425e-01 -8.52966681e-02 -2.37591304e-02\n",
      " -4.94187437e-02  1.51652293e-02  4.36946675e-02  4.03097738e-03\n",
      " -1.35321796e-01 -1.08326234e-01 -5.00398390e-02 -8.72096717e-02\n",
      "  3.68847027e-02  7.33072907e-02 -7.12994393e-03  1.96398553e-02\n",
      " -5.49772196e-02  1.28277363e-02  1.39971729e-02 -8.48656669e-02\n",
      "  7.26049691e-02  1.08923558e-02 -2.14516986e-02 -1.51581854e-01\n",
      "  1.02240155e-02  1.07216023e-01 -1.26954690e-02  2.27646027e-02\n",
      "  1.73321441e-02 -5.00799678e-02 -4.09060679e-02 -8.83507058e-02\n",
      "  3.94340083e-02  4.51690592e-02  2.02759113e-02 -8.49575177e-02\n",
      "  8.29858705e-03  1.18506730e-01  1.65496230e-01 -1.06485840e-03\n",
      "  1.45920753e-01 -1.50656372e-01 -1.44341856e-01  4.90642451e-02\n",
      "  9.06786248e-02 -1.86373200e-02  1.08055167e-01 -9.98383835e-02\n",
      "  2.56936848e-01  4.99435477e-02  2.53343314e-01  7.66233131e-02\n",
      "  3.89410518e-02 -5.88317066e-02 -6.55004336e-03 -2.71510910e-02\n",
      "  1.28851250e-01  2.44352259e-02 -2.47415751e-01  1.25765234e-01\n",
      " -4.28203084e-02 -2.78115243e-04  7.38523006e-02  8.43832716e-02\n",
      "  4.59044501e-02 -6.05779365e-02  1.61012605e-01 -5.82349077e-02\n",
      "  1.18244179e-01  1.16720200e-01 -7.67921621e-04  6.31724671e-02\n",
      "  1.34080976e-01 -6.94224015e-02 -3.96623947e-02 -1.24392763e-01\n",
      "  2.49060728e-02  1.69511095e-01  1.83200240e-01  1.93455964e-01\n",
      " -1.19098023e-01 -9.48684886e-02  3.95624824e-02 -8.25121999e-02\n",
      " -4.78657596e-02  4.48019654e-02 -1.67994928e-02 -1.63596813e-02\n",
      "  7.52644986e-02  4.36912803e-03 -4.47877422e-02  9.86158773e-02\n",
      "  5.57952002e-02 -1.17557816e-01  4.01387773e-02  1.30816534e-01\n",
      "  4.24615629e-02  1.68534935e-01  1.22613512e-01  8.75670835e-02\n",
      "  9.40159410e-02 -1.64724082e-01 -1.05554111e-01 -2.95076277e-02\n",
      " -4.67873290e-02 -8.28620270e-02  1.15770228e-01  3.09962817e-02\n",
      "  1.59440368e-01 -2.28464141e-01 -7.08214939e-04  6.76251128e-02\n",
      " -6.70402944e-02  7.23384917e-02  1.77657846e-02 -6.60256743e-02\n",
      " -1.04137190e-01 -3.37348320e-02 -7.08642602e-02  1.35218725e-01\n",
      " -1.04490839e-01 -1.07609503e-01  7.60486573e-02 -1.73167586e-01\n",
      " -1.87075585e-01  2.42775902e-01  8.64100233e-02 -2.31099073e-02\n",
      "  1.27173051e-01  8.94289613e-02 -7.38087222e-02  4.09083664e-02\n",
      "  5.06109446e-02  1.48725174e-02 -2.84799039e-02 -3.46324444e-02\n",
      "  1.33957472e-02  1.27518192e-01  3.38870548e-02  1.17738828e-01\n",
      "  1.34852156e-01 -8.62496495e-02 -7.98362419e-02  5.06983362e-02\n",
      "  5.52473068e-02 -8.20799246e-02 -1.24075830e-01 -9.77848694e-02\n",
      " -1.41627535e-01  2.31917113e-01  8.53823572e-02 -4.90125604e-02\n",
      "  2.38676686e-02 -1.72581039e-02 -7.03232214e-02 -7.47146383e-02\n",
      "  2.93732435e-03 -9.56458896e-02 -1.07253201e-01 -7.92833976e-03\n",
      " -3.40876877e-02  1.08294092e-01 -4.68641967e-02 -2.69399881e-02\n",
      "  3.42593491e-02 -4.33676615e-02 -7.47215375e-02  9.37794894e-02\n",
      "  1.17842212e-01  2.86346544e-02  8.51464421e-02  5.57566155e-03\n",
      "  4.05288488e-02  9.23665985e-02 -9.41316858e-02  2.62299576e-03\n",
      " -3.97559442e-03 -6.45573139e-02 -1.60054773e-01 -1.83850750e-01\n",
      "  9.01394561e-02 -1.25662601e-02 -5.65492809e-02 -1.37352869e-01\n",
      " -5.27102500e-02  7.72454888e-02 -1.13016348e-02  6.90862015e-02\n",
      " -2.76204273e-02 -2.13969816e-02  7.23863915e-02  9.90640894e-02\n",
      "  6.53653406e-03 -2.21311990e-02  6.82068169e-02  6.06586672e-02\n",
      "  1.03967942e-01  6.31838590e-02  6.64735213e-02  6.83403909e-02\n",
      "  6.38551190e-02  8.17525014e-02 -1.09101385e-01  3.45391892e-02\n",
      " -1.00986257e-01  2.30107252e-02 -3.82496864e-02 -5.96697368e-02\n",
      " -1.35268988e-02  7.73912445e-02  9.75111779e-03 -1.05525933e-01\n",
      " -4.61617373e-02 -8.75345469e-02  5.61760776e-02 -1.25832856e-01\n",
      "  3.00072804e-02  7.21240416e-02  4.60559018e-02 -4.81277555e-02\n",
      "  1.19738551e-02  1.79845316e-03  6.48624673e-02 -3.10106035e-02\n",
      " -6.19810708e-02 -7.82183930e-02 -1.39559790e-01  2.76228879e-02\n",
      "  8.63622222e-03  3.85440886e-02 -8.36719275e-02 -9.81520116e-02\n",
      " -8.22815578e-03  2.31957436e-02 -8.26201681e-03  5.72267100e-02\n",
      "  1.45591810e-01  5.96668907e-02  7.30446633e-03  4.79177050e-02\n",
      "  9.01075974e-02 -4.53736149e-02  3.41530666e-02 -6.35438710e-02\n",
      "  9.63032618e-02 -3.40146497e-02  7.94983804e-02  4.62395586e-02\n",
      "  8.16449057e-03 -2.74781976e-02 -7.29938969e-02  1.74786240e-01\n",
      " -8.48876014e-02  1.18082920e-02  7.71974549e-02 -1.56462546e-02\n",
      "  2.45742705e-02 -4.42612432e-02  7.33880997e-02  2.07121056e-02\n",
      " -1.54169800e-03  4.48633544e-02  9.09414664e-02  1.23942621e-01\n",
      " -1.55686280e-02  8.53055269e-02  5.39566837e-02  5.06916232e-02\n",
      "  6.83704540e-02  3.24871726e-02 -4.45189327e-03  1.21758230e-01\n",
      "  1.24988712e-01  7.63873607e-02  3.04565802e-02 -2.37922654e-01\n",
      " -8.77606720e-02  1.04000904e-02 -1.80880968e-02 -1.67529583e-01\n",
      " -8.80722478e-02  1.11973293e-01 -1.33214891e-01  3.37997898e-02\n",
      " -8.70596468e-02 -2.09901202e-02  4.43611629e-02  5.87785281e-02\n",
      "  4.14300300e-02  3.68457511e-02  2.44405996e-02  6.54601753e-02\n",
      "  2.81375676e-01 -6.58344403e-02 -9.20590833e-02 -1.45294545e-02\n",
      " -7.52704870e-03  4.82470281e-02 -2.61897128e-02 -9.61934626e-02\n",
      "  7.68835694e-02 -4.06956263e-02  9.02532116e-02  1.21300921e-01\n",
      " -6.54800385e-02  7.29388446e-02 -1.43780574e-01  1.02566727e-01\n",
      "  5.14395759e-02  2.67853998e-02 -1.36566311e-02 -1.22592591e-01\n",
      " -1.33356405e-02 -3.25637013e-02  5.48640154e-02  1.51254207e-01\n",
      "  5.39481640e-02 -2.08221108e-01 -2.98861470e-02  1.17506959e-01\n",
      "  4.09860387e-02 -9.14054289e-02 -1.03506513e-01  1.26716033e-01\n",
      "  5.68807721e-02 -1.28172949e-01 -1.53114572e-01  2.52975058e-03\n",
      "  2.89381575e-02  7.74443820e-02 -2.92125363e-02 -1.92700475e-01\n",
      " -1.80696920e-02 -1.09188654e-01  4.05642483e-03 -9.45341736e-02\n",
      "  1.41401291e-02  9.80697721e-02  1.15088500e-01  1.05840070e-02\n",
      "  9.90669653e-02 -9.51393321e-02 -1.21916071e-01  5.96432649e-02\n",
      "  1.86663345e-01  1.16570115e-01  9.89482403e-02  5.25024161e-02\n",
      " -9.55196470e-02  4.94649112e-02  2.96810213e-02  9.13328230e-02\n",
      "  1.08395718e-01 -7.10888952e-02  2.31962055e-02 -9.98922884e-02\n",
      "  3.86784486e-02 -2.54299715e-02 -6.01983666e-02  2.26319917e-02\n",
      " -2.65860651e-02  1.76413566e-01  1.25305504e-01 -5.11981398e-02\n",
      "  1.87594458e-01 -6.64422810e-02 -1.89493876e-03  4.07761000e-02\n",
      " -2.67600100e-02  3.52570713e-02 -4.34756419e-03 -8.39107186e-02\n",
      " -3.24794017e-02 -5.12477476e-03  2.68811476e-03 -2.10714955e-02\n",
      "  1.29752502e-01 -5.36737293e-02 -6.19419850e-03  9.86966714e-02\n",
      " -5.55604137e-02  8.87727067e-02  2.98986360e-02  9.01257247e-03\n",
      " -9.14056748e-02 -1.56978052e-02 -1.20017253e-01  6.10112846e-02\n",
      "  3.42311636e-02 -1.12924492e-02 -2.97768295e-01  1.09218307e-01\n",
      " -7.11274147e-02 -1.31476939e-01  6.12616539e-02  4.55056615e-02\n",
      "  6.92952722e-02  8.15690532e-02 -2.14765724e-02  1.06593939e-02\n",
      "  1.12330690e-01  1.87389344e-01  1.68865561e-01  2.08792448e-01\n",
      "  7.39661530e-02  5.58306612e-02  2.76965275e-02 -8.65770280e-02\n",
      "  5.79885691e-02  8.32027718e-02 -3.05656772e-02  3.27753872e-02\n",
      "  2.03600619e-02 -8.46928954e-02 -1.57541856e-01 -2.30662629e-01\n",
      " -1.58884469e-02  6.84885234e-02 -1.04345053e-01 -8.78962204e-02\n",
      "  5.17108757e-03  5.48025705e-02 -4.79983836e-02 -6.95787743e-02\n",
      "  1.29118383e-01 -3.96033144e-03 -1.39699280e-02  1.40523911e-02\n",
      "  4.78271628e-03 -2.37365216e-02  2.07797959e-02 -1.60675067e-02]\n"
     ]
    }
   ],
   "source": [
    "vector_sample = model.infer_vector(test_corpus[1])\n",
    "print(\"Tokenized test sample:\")\n",
    "print(test_corpus[1])\n",
    "print(\"\\nInferred vector:\")\n",
    "print(vector_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For more:\n",
    "* [Yaron Vazana](http://yaronvazana.com/2018/01/20/training-doc2vec-model-with-gensim/)\n",
    "* [Rare Technologies](https://rare-technologies.com/doc2vec-tutorial/)\n",
    "* [Gensim Documentation](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "* [Gensim Doc2Vec Tutorial on the IMDB Sentiment Dataset](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)\n",
    "* [Doc2Vec Tutorial on the Lee Dataset](https://markroxor.github.io/gensim/static/notebooks/doc2vec-lee.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
